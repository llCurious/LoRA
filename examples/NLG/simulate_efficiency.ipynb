{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path\n",
    "sys.path.append(\"/home/haoqi.whq/llm-inference/LoRA\")\n",
    "\n",
    "from src.model import GPT2Config, GPT2LMModel\n",
    "import torch\n",
    "from loralib import PruneLayer, LoRALayer\n",
    "import loralib as lora\n",
    "import copy\n",
    "from torch import nn\n",
    "from src.model import LayerNorm, Conv1D\n",
    "import time\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config(\n",
    "    n_embd=1024,\n",
    "    n_layer=24,\n",
    "    n_head=16,\n",
    "    lora_attn_dim=4,\n",
    "    lora_attn_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    enable_mlp=True,\n",
    "    enable_wo=True,\n",
    "    enable_wq=True,\n",
    "    enable_wk=True,\n",
    "    enable_wv=True,\n",
    ")\n",
    "\n",
    "B = 8\n",
    "SEQ_LEN = 512\n",
    "\n",
    "NON_LINEAR_TIME, IO_TIME, MASK_TIME, LINER_TIME_GPU, LORA_TIME_TEE = 0, 0, 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(x):\n",
    "    r = torch.zeros_like(x)\n",
    "    return x + r\n",
    "\n",
    "def unmask(x):\n",
    "    r = torch.zeros_like(x)\n",
    "    return x - r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer block runtime breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTConv1D(nn.Module, LoRALayer):\n",
    "    # LoRA implemented in a Conv1D layer\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        r: int = 0,\n",
    "        lora_alpha: int = 1,\n",
    "        lora_dropout: float = 0.0,\n",
    "        fan_in_fan_out: bool = False,  # Set this to True if the layer to replace stores weight like (fan_in, fan_out)\n",
    "        merge_weights: bool = True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(GPTConv1D, self).__init__()\n",
    "        LoRALayer.__init__(\n",
    "            self,\n",
    "            r=r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=lora_dropout,\n",
    "            merge_weights=merge_weights,\n",
    "        )\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.fan_in_fan_out = fan_in_fan_out\n",
    "        w = torch.empty(in_features, out_features)\n",
    "        nn.init.normal_(w, std=0.02)\n",
    "        self.weight = Parameter(w)\n",
    "        self.bias = Parameter(torch.zeros(out_features))\n",
    "\n",
    "        # Actual trainable parameters\n",
    "        if r > 0:\n",
    "            self.lora_A = nn.Parameter(self.weight.new_zeros((r, in_features)))\n",
    "            self.lora_B = nn.Parameter(self.weight.new_zeros((out_features, r)))\n",
    "            # self.lora_scaling = self.lora_alpha / self.r\n",
    "            self.lora_scaling = nn.Parameter(torch.tensor(self.lora_alpha / self.r))\n",
    "\n",
    "            # Freezing the pre-trained weight matrix\n",
    "            self.weight.requires_grad = False\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.weight, std=0.02)\n",
    "        nn.init.zeros_(self.bias)\n",
    "        if hasattr(self, \"lora_A\"):\n",
    "            # initialize A the same way as the default for nn.Linear and B to zero\n",
    "            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.lora_B)\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        def T(w):\n",
    "            return w.transpose(0, 1) if self.fan_in_fan_out else w\n",
    "\n",
    "        nn.Linear.train(self, mode)\n",
    "        if mode:\n",
    "            if self.merge_weights and self.merged:\n",
    "                # Make sure that the weights are not merged\n",
    "                if self.r > 0:\n",
    "                    self.weight.data -= T(self.lora_B @ self.lora_A) * self.lora_scaling\n",
    "                self.merged = False\n",
    "        else:\n",
    "            if self.merge_weights and not self.merged:\n",
    "                # Merge the weights and mark it\n",
    "                if self.r > 0:\n",
    "                    self.weight.data += T(self.lora_B @ self.lora_A) * self.lora_scaling\n",
    "                self.merged = True\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        size_out = x.size()[:-1] + (self.out_features,)\n",
    "\n",
    "        start = time.time()\n",
    "        x = mask(x)\n",
    "        end = time.time()\n",
    "        print(f\"\\t=====[MLP]====== mask x {(end - start) * 1000} ms\")\n",
    "        global MASK_TIME\n",
    "        MASK_TIME += (end - start) * 1000\n",
    "\n",
    "        start = time.time()\n",
    "        x = x.cuda()\n",
    "        end = time.time()\n",
    "        print(f\"\\t=====[MLP]====== cpu->gpu IO {(end - start) * 1000} ms\")\n",
    "        global IO_TIME\n",
    "        IO_TIME += (end - start) * 1000\n",
    "        \n",
    "        self.weight = Parameter(self.weight.cuda())\n",
    "        self.bias = Parameter(self.bias.cuda())\n",
    "\n",
    "        start = time.time()\n",
    "        result = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
    "        result = result.view(*size_out)\n",
    "        end = time.time()\n",
    "        print(f\"\\t=====[MLP]====== gpu linear {(end - start) * 1000} ms\")\n",
    "        global LINER_TIME_GPU\n",
    "        LINER_TIME_GPU += (end - start) * 1000\n",
    "\n",
    "        start = time.time()\n",
    "        result = result.cpu()\n",
    "        end = time.time()\n",
    "        print(f\"\\t=====[MLP]====== gpu->cpu IO {(end - start) * 1000} ms\")\n",
    "        IO_TIME += (end - start) * 1000\n",
    "\n",
    "        start = time.time()\n",
    "        result = unmask(result)\n",
    "        end = time.time()\n",
    "        print(f\"\\t=====[MLP]====== unmask xA {(end - start) * 1000} ms\")\n",
    "        MASK_TIME += (end - start) * 1000\n",
    "\n",
    "        x = x.cpu()\n",
    "        start = time.time()\n",
    "        lora_res = (\n",
    "            self.lora_dropout(x)\n",
    "            @ self.lora_A.transpose(0, 1)\n",
    "            @ self.lora_B.transpose(0, 1)\n",
    "        ) * self.lora_scaling\n",
    "        end = time.time()\n",
    "        print(f\"\\t=====[MLP]====== LoRA linear {(end - start) * 1000} ms\")\n",
    "        global LORA_TIME_TEE\n",
    "        LORA_TIME_TEE += (end - start) * 1000\n",
    "\n",
    "        if self.r > 0 and not self.merged:\n",
    "            result += lora_res\n",
    "        return result\n",
    "    \n",
    "class Linear(nn.Linear, LoRALayer):\n",
    "    # LoRA implemented in a dense layer\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        r: int = 0,\n",
    "        lora_alpha: int = 1,\n",
    "        lora_dropout: float = 0.0,\n",
    "        fan_in_fan_out: bool = False,  # Set this to True if the layer to replace stores weight like (fan_in, fan_out)\n",
    "        merge_weights: bool = True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        nn.Linear.__init__(self, in_features, out_features, **kwargs)\n",
    "        LoRALayer.__init__(\n",
    "            self,\n",
    "            r=r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=lora_dropout,\n",
    "            merge_weights=merge_weights,\n",
    "        )\n",
    "\n",
    "        self.fan_in_fan_out = fan_in_fan_out\n",
    "        # Actual trainable parameters\n",
    "        if r > 0:\n",
    "            self.lora_A = nn.Parameter(self.weight.new_zeros((r, in_features)))\n",
    "            self.lora_B = nn.Parameter(self.weight.new_zeros((out_features, r)))\n",
    "            # self.scaling = self.lora_alpha / self.r\n",
    "            self.lora_scaling = nn.Parameter(torch.tensor(self.lora_alpha / self.r))\n",
    "\n",
    "            # Freezing the pre-trained weight matrix\n",
    "            self.weight.requires_grad = False\n",
    "        self.reset_parameters()\n",
    "        if fan_in_fan_out:\n",
    "            self.weight.data = self.weight.data.transpose(0, 1)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.Linear.reset_parameters(self)\n",
    "        if hasattr(self, \"lora_A\"):\n",
    "            # initialize A the same way as the default for nn.Linear and B to zero\n",
    "            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.lora_B)\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        def T(w):\n",
    "            return w.transpose(0, 1) if self.fan_in_fan_out else w\n",
    "\n",
    "        nn.Linear.train(self, mode)\n",
    "        if mode:\n",
    "            if self.merge_weights and self.merged:\n",
    "                # Make sure that the weights are not merged\n",
    "                if self.r > 0:\n",
    "                    self.weight.data -= T(self.lora_B @ self.lora_A) * self.lora_scaling\n",
    "                self.merged = False\n",
    "        else:\n",
    "            if self.merge_weights and not self.merged:\n",
    "                # Merge the weights and mark it\n",
    "                if self.r > 0:\n",
    "                    self.weight.data += T(self.lora_B @ self.lora_A) * self.lora_scaling\n",
    "                self.merged = True\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        def T(w):\n",
    "            return w.transpose(0, 1) if self.fan_in_fan_out else w\n",
    "\n",
    "        if self.r > 0 and not self.merged:\n",
    "\n",
    "            start = time.time()\n",
    "            x = mask(x)\n",
    "            end = time.time()\n",
    "            print(f\"\\t=====[Attention]====== mask x {(end - start) * 1000} ms\")\n",
    "            global MASK_TIME\n",
    "            MASK_TIME += (end - start) * 1000\n",
    "\n",
    "            start = time.time()\n",
    "            x = x.cuda()\n",
    "            end = time.time()\n",
    "            print(f\"\\t=====[Attention]====== cpu->gpu IO {(end - start) * 1000} ms\")\n",
    "            global IO_TIME\n",
    "            IO_TIME += (end - start) * 1000\n",
    "\n",
    "            self.weight = Parameter(self.weight.cuda())\n",
    "            self.bias = Parameter(self.bias.cuda())\n",
    "            \n",
    "            start = time.time()\n",
    "            result = F.linear(x, T(self.weight), bias=self.bias)\n",
    "            end = time.time()\n",
    "            print(f\"\\t=====[Attention]====== GPU linear {(end - start) * 1000} ms\")\n",
    "            global LINER_TIME_GPU\n",
    "            LINER_TIME_GPU += (end - start) * 1000\n",
    "\n",
    "            start = time.time()\n",
    "            result = result.cpu()\n",
    "            end = time.time()\n",
    "            print(f\"\\t=====[Attention]====== gpu->cpu IO {(end - start) * 1000} ms\")\n",
    "            IO_TIME += (end - start) * 1000\n",
    "\n",
    "            start = time.time()\n",
    "            result = unmask(result)\n",
    "            end = time.time()\n",
    "            print(f\"\\t=====[Attention]====== unmask xA {(end - start) * 1000} ms\")\n",
    "            MASK_TIME += (end - start) * 1000\n",
    "\n",
    "            x = x.cpu()\n",
    "            start = time.time()\n",
    "            lora_res = (\n",
    "                self.lora_dropout(x)\n",
    "                @ self.lora_A.transpose(0, 1)\n",
    "                @ self.lora_B.transpose(0, 1)\n",
    "            ) * self.lora_scaling\n",
    "            end = time.time()\n",
    "            print(f\"\\t=====[Attention]====== LoRA linear {(end - start) * 1000} ms\")\n",
    "            global LORA_TIME_TEE\n",
    "            LORA_TIME_TEE += (end - start) * 1000\n",
    "\n",
    "            result += lora_res\n",
    "            return result\n",
    "        else:\n",
    "            return F.linear(x, T(self.weight), bias=self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PruneLinear(PruneLayer, Linear):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        r: int = 0,\n",
    "        lora_alpha: int = 1,\n",
    "        lora_dropout: float = 0,\n",
    "        fan_in_fan_out: bool = False,\n",
    "        merge_weights: bool = True,\n",
    "        keep_flag: bool = True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        PruneLayer.__init__(self, keep_flag)\n",
    "        Linear.__init__(\n",
    "            self,\n",
    "            in_features,\n",
    "            out_features,\n",
    "            r,\n",
    "            lora_alpha,\n",
    "            lora_dropout,\n",
    "            fan_in_fan_out,\n",
    "            merge_weights,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # update scaling as thr\n",
    "        # self.scaling = nn.Parameter(torch.tensor(self.lora_alpha / self.r))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if not self.keep_flag:\n",
    "            self.merged = True  # set merged to True to escape computing lora module\n",
    "        return Linear.forward(self, x)\n",
    "\n",
    "    def complexity(self):\n",
    "        return self.lora_scaling * (self.r * self.in_features + self.out_features * self.r)\n",
    "\n",
    "    def empirical_consumption(self, hardwares):\n",
    "        return self.complexity()\n",
    "\n",
    "class PruneGPTConv1D(PruneLayer, GPTConv1D):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        r: int = 0,\n",
    "        lora_alpha: int = 1,\n",
    "        lora_dropout: float = 0,\n",
    "        fan_in_fan_out: bool = False,\n",
    "        merge_weights: bool = True,\n",
    "        keep_flag: bool = True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        PruneLayer.__init__(self, keep_flag)\n",
    "        GPTConv1D.__init__(\n",
    "            self,\n",
    "            in_features,\n",
    "            out_features,\n",
    "            r,\n",
    "            lora_alpha,\n",
    "            lora_dropout,\n",
    "            fan_in_fan_out,\n",
    "            merge_weights,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # update scaling as thr\n",
    "        # self.scaling = nn.Parameter(torch.tensor(self.lora_alpha / self.r))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if not self.keep_flag:\n",
    "            self.merged = True  # set merged to True to escape computing lora module\n",
    "        return GPTConv1D.forward(self, x)\n",
    "\n",
    "    def complexity(self):\n",
    "        return self.lora_scaling * (self.r * self.in_features + self.out_features * self.r)\n",
    "\n",
    "    def empirical_consumption(self, hardwares):\n",
    "        return self.complexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, nx, n_ctx, config, scale=False):\n",
    "        super(Attention, self).__init__()\n",
    "        n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n",
    "        # [switch nx => n_state from Block to Attention to keep identical to TF implem]\n",
    "\n",
    "        assert n_state % config.n_head == 0\n",
    "        self.register_buffer(\n",
    "            \"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx)\n",
    "        )\n",
    "        self.n_head = config.n_head\n",
    "        self.split_size = n_state\n",
    "        self.scale = scale\n",
    "\n",
    "        if config.enable_wq:\n",
    "            self.q_attn = PruneLinear(\n",
    "                nx,\n",
    "                n_state,\n",
    "                r=config.lora_attn_dim,\n",
    "                lora_alpha=config.lora_attn_alpha,\n",
    "                lora_dropout=config.lora_dropout,\n",
    "                fan_in_fan_out=True,\n",
    "                merge_weights=False,\n",
    "            )\n",
    "        else:\n",
    "            self.q_attn = nn.Linear(nx, n_state)\n",
    "\n",
    "        if config.enable_wk:\n",
    "            self.k_attn = PruneLinear(\n",
    "                nx,\n",
    "                n_state,\n",
    "                r=config.lora_attn_dim,\n",
    "                lora_alpha=config.lora_attn_alpha,\n",
    "                lora_dropout=config.lora_dropout,\n",
    "                fan_in_fan_out=True,\n",
    "                merge_weights=False,\n",
    "            )\n",
    "        else:\n",
    "            self.k_attn = nn.Linear(nx, n_state)\n",
    "\n",
    "        if config.enable_wv:\n",
    "            self.v_attn = PruneLinear(\n",
    "                nx,\n",
    "                n_state,\n",
    "                r=config.lora_attn_dim,\n",
    "                lora_alpha=config.lora_attn_alpha,\n",
    "                lora_dropout=config.lora_dropout,\n",
    "                fan_in_fan_out=True,\n",
    "                merge_weights=False,\n",
    "            )\n",
    "        else:\n",
    "            self.v_attn = nn.Linear(nx, n_state)\n",
    "        # self.c_attn = lora.MergedLinear(\n",
    "        #     nx,\n",
    "        #     n_state * 3,\n",
    "        #     r=config.lora_attn_dim,\n",
    "        #     lora_alpha=config.lora_attn_alpha,\n",
    "        #     lora_dropout=config.lora_dropout,\n",
    "        #     enable_lora=[config.enable_wq, config.enable_wk, config.enable_wv],\n",
    "        #     fan_in_fan_out=True,\n",
    "        #     merge_weights=False,\n",
    "        # )\n",
    "        # print(\n",
    "        #     f\"QKV Attention LoRA ({[config.enable_wq, config.enable_wk, config.enable_wv]}): {self.c_attn}\"\n",
    "        # )\n",
    "\n",
    "        if not config.enable_wo:\n",
    "            self.c_proj = Conv1D(n_state, nx)\n",
    "            print(f\"O Attention not use LoRA: {self.c_proj}\")\n",
    "        else:\n",
    "            self.c_proj = PruneGPTConv1D(\n",
    "                in_features=nx,\n",
    "                out_features=n_state,\n",
    "                r=config.lora_attn_dim,\n",
    "                lora_alpha=config.lora_attn_alpha,\n",
    "                lora_dropout=config.lora_dropout,\n",
    "                fan_in_fan_out=False,\n",
    "                merge_weights=False,\n",
    "                keep_flag=True,\n",
    "            )\n",
    "            print(f\"O Attention using LoRA: {self.c_proj}\")\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "    def _attn(self, q, k, v, len_kv=None):\n",
    "        w = torch.matmul(q, k)\n",
    "        if self.scale:\n",
    "            w = w / math.sqrt(v.size(-1))\n",
    "        nd, ns = w.size(-2), w.size(-1)\n",
    "        b = self.bias[:, :, ns - nd : ns, :ns]\n",
    "        w = w * b - 1e10 * (1 - b)\n",
    "\n",
    "        # q : (batch, head, q_seq_length, head_features)\n",
    "        # k : (batch, head, head_features, kv_seq_length)\n",
    "        # w : (batch, head, q_seq_length, kv_seq_length)\n",
    "        # v : (batch, head, kv_seq_length, head_features)\n",
    "        if len_kv is not None:\n",
    "            _len = torch.arange(k.size(-1), device=k.device)\n",
    "            _input_msk = _len[None, :] >= (len_kv)[:, None]\n",
    "            w = w.masked_fill(_input_msk.unsqueeze(1).unsqueeze(2), -1.0e10)\n",
    "\n",
    "        w = nn.Softmax(dim=-1)(w)\n",
    "        return torch.matmul(w, v)\n",
    "\n",
    "    def merge_heads(self, x):\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)\n",
    "        return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states\n",
    "\n",
    "    def split_heads(self, x, k=False):\n",
    "        new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)\n",
    "        x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states\n",
    "        if k:\n",
    "            return x.permute(\n",
    "                0, 2, 3, 1\n",
    "            ).contiguous()  # (batch, head, head_features, seq_length)\n",
    "        else:\n",
    "            return x.permute(\n",
    "                0, 2, 1, 3\n",
    "            ).contiguous()  # (batch, head, seq_length, head_features)\n",
    "\n",
    "    def forward(self, x, history=None, layer_past=None, len_past=None):\n",
    "        hidden_states = x\n",
    "\n",
    "        \n",
    "        start = time.time()\n",
    "        query, key, value = self.q_attn(x), self.k_attn(x), self.v_attn(x)\n",
    "        print(f\"qkv linear {(time.time() - start) * 1000} ms\")\n",
    "\n",
    "        query = self.split_heads(query)\n",
    "        key = self.split_heads(key, k=True)\n",
    "        value = self.split_heads(value)\n",
    "\n",
    "        # _input_msk = None\n",
    "\n",
    "        len_kv = None\n",
    "\n",
    "        if layer_past is not None:\n",
    "            # key : (batch, head, head_features, seq_length)\n",
    "            # value : (batch, head, seq_length, head_features)\n",
    "            # layer_past, key : (batch, head, seq_length, head_features)\n",
    "            if len_past is None:\n",
    "                past_key, past_value = (\n",
    "                    layer_past[0].transpose(-2, -1),\n",
    "                    layer_past[1],\n",
    "                )  # transpose back cf below\n",
    "                key = torch.cat((past_key, key), dim=-1)\n",
    "                value = torch.cat((past_value, value), dim=-2)\n",
    "            else:\n",
    "                key_seq = key.shape[-1]\n",
    "                assert key_seq == 1\n",
    "\n",
    "                _batch = torch.arange(\n",
    "                    0, key.shape[0], dtype=torch.long, device=key.device\n",
    "                )\n",
    "\n",
    "                past_key, past_value = layer_past[0], layer_past[1]\n",
    "\n",
    "                past_key[_batch, :, len_past, :] = key.squeeze(-1)\n",
    "                past_value[_batch, :, len_past, :] = value.squeeze(-2)\n",
    "\n",
    "                key = past_key.transpose(-2, -1)\n",
    "                value = past_value\n",
    "\n",
    "                len_kv = len_past + 1\n",
    "\n",
    "        present = torch.stack(\n",
    "            (key.transpose(-2, -1), value)\n",
    "        )  # transpose to have same shapes for stacking\n",
    "\n",
    "        start = time.time()\n",
    "        a = self._attn(query, key, value, len_kv=len_kv)\n",
    "        print(f\"bmm+softmax+bmm {(time.time() - start) * 1000} ms\")\n",
    "\n",
    "        start = time.time()\n",
    "        a = self.merge_heads(a)\n",
    "        print(f\"merge heads {(time.time() - start) * 1000} ms\")\n",
    "\n",
    "        start = time.time()\n",
    "        a = self.c_proj(a)\n",
    "        print(f\"o linear {(time.time() - start) * 1000} ms\")\n",
    "\n",
    "        return a, present\n",
    "    \n",
    "\n",
    "def gelu(x):\n",
    "    return (\n",
    "        0.5\n",
    "        * x\n",
    "        * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    )\n",
    "\n",
    "def gelu_fast(x):\n",
    "    return 0.5 * x * (1.0 + torch.tanh(x * 0.7978845608 * (1.0 + 0.044715 * x * x)))\n",
    "\n",
    "def gelu_impl(x):\n",
    "    \"\"\"OpenAI's gelu implementation.\"\"\"\n",
    "    return (\n",
    "        0.5 * x * (1.0 + torch.tanh(0.7978845608028654 * x * (1.0 + 0.044715 * x * x)))\n",
    "    )\n",
    "\n",
    "def gelu_quad(x):\n",
    "    return 0.125 * torch.square(x + 0.25 * x + 0.5)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_state, config):  # in MLP: n_state=3072 (4 * n_embd)\n",
    "        super(MLP, self).__init__()\n",
    "        nx = config.n_embd\n",
    "        # raw\n",
    "        if not config.enable_mlp:\n",
    "            self.c_fc = Conv1D(n_state, nx)\n",
    "            self.c_proj = Conv1D(nx, n_state)\n",
    "            print(f\"MLP not use LoRA: {self.c_fc}, {self.c_proj}\")\n",
    "        else:  # modified\n",
    "            self.c_fc = PruneGPTConv1D(\n",
    "                in_features=nx,\n",
    "                out_features=n_state,\n",
    "                r=config.lora_attn_dim,\n",
    "                lora_alpha=config.lora_attn_alpha,\n",
    "                lora_dropout=config.lora_dropout,\n",
    "                fan_in_fan_out=False,\n",
    "                merge_weights=False,\n",
    "                keep_flag=True,\n",
    "            )\n",
    "            self.c_proj = PruneGPTConv1D(\n",
    "                in_features=n_state,\n",
    "                out_features=nx,\n",
    "                r=config.lora_attn_dim,\n",
    "                lora_alpha=config.lora_attn_alpha,\n",
    "                lora_dropout=config.lora_dropout,\n",
    "                fan_in_fan_out=False,\n",
    "                merge_weights=False,\n",
    "                keep_flag=True,\n",
    "            )\n",
    "            print(f\"MLP using LoRA: {self.c_fc}, {self.c_proj}\")\n",
    "\n",
    "        self.act = gelu_fast\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.c_fc(x)\n",
    "\n",
    "        start = time.time()\n",
    "        h1 = self.act(h)\n",
    "        end = time.time()\n",
    "        # print(h.shape)\n",
    "        print(f\"GeLU nonlinear TEE time {(end - start) * 1000} ms\")\n",
    "\n",
    "        # h = h.cuda()\n",
    "        # start = time.time()\n",
    "        # h1 = self.act(h)\n",
    "        # end = time.time()\n",
    "        # print(f\"GeLU nonlinear GPU time {(end - start) * 1000} ms\")\n",
    "        \n",
    "        global NON_LINEAR_TIME\n",
    "        NON_LINEAR_TIME += (end - start) * 1000\n",
    "\n",
    "        # h1 = h1.cpu()\n",
    "        h2 = self.c_proj(h1)\n",
    "        return h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Attention using LoRA: PruneGPTConv1D(\n",
      "  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "MLP using LoRA: PruneGPTConv1D(\n",
      "  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "), PruneGPTConv1D(\n",
      "  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "ln_1 3.737926483154297 ms\n",
      "\t=====[Attention]====== mask x 1.1548995971679688 ms\n",
      "\t=====[Attention]====== cpu->gpu IO 2.8755664825439453 ms\n",
      "\t=====[Attention]====== GPU linear 0.23245811462402344 ms\n",
      "\t=====[Attention]====== gpu->cpu IO 3.907442092895508 ms\n",
      "\t=====[Attention]====== unmask xA 0.980377197265625 ms\n",
      "\t=====[Attention]====== LoRA linear 1.4777183532714844 ms\n",
      "\t=====[Attention]====== mask x 1.1882781982421875 ms\n",
      "\t=====[Attention]====== cpu->gpu IO 2.804279327392578 ms\n",
      "\t=====[Attention]====== GPU linear 0.20360946655273438 ms\n",
      "\t=====[Attention]====== gpu->cpu IO 4.20689582824707 ms\n",
      "\t=====[Attention]====== unmask xA 1.1839866638183594 ms\n",
      "\t=====[Attention]====== LoRA linear 1.7919540405273438 ms\n",
      "\t=====[Attention]====== mask x 0.9145736694335938 ms\n",
      "\t=====[Attention]====== cpu->gpu IO 3.0663013458251953 ms\n",
      "\t=====[Attention]====== GPU linear 0.21839141845703125 ms\n",
      "\t=====[Attention]====== gpu->cpu IO 4.460811614990234 ms\n",
      "\t=====[Attention]====== unmask xA 1.018524169921875 ms\n",
      "\t=====[Attention]====== LoRA linear 1.642465591430664 ms\n",
      "qkv linear 46.0052490234375 ms\n",
      "bmm+softmax+bmm 63.253164291381836 ms\n",
      "merge heads 0.9317398071289062 ms\n",
      "\t=====[MLP]====== mask x 0.9341239929199219 ms\n",
      "\t=====[MLP]====== cpu->gpu IO 2.8793811798095703 ms\n",
      "\t=====[MLP]====== gpu linear 0.1785755157470703 ms\n",
      "\t=====[MLP]====== gpu->cpu IO 4.289865493774414 ms\n",
      "\t=====[MLP]====== unmask xA 0.9629726409912109 ms\n",
      "\t=====[MLP]====== LoRA linear 1.9533634185791016 ms\n",
      "o linear 15.317916870117188 ms\n",
      "attention 131.25371932983398 ms\n",
      "ln_2 5.640745162963867 ms\n",
      "\t=====[MLP]====== mask x 1.821279525756836 ms\n",
      "\t=====[MLP]====== cpu->gpu IO 2.915620803833008 ms\n",
      "\t=====[MLP]====== gpu linear 0.20623207092285156 ms\n",
      "\t=====[MLP]====== gpu->cpu IO 66.37883186340332 ms\n",
      "\t=====[MLP]====== unmask xA 18.32413673400879 ms\n",
      "\t=====[MLP]====== LoRA linear 12.53199577331543 ms\n",
      "GeLU nonlinear TEE time 57.7847957611084 ms\n",
      "\t=====[MLP]====== mask x 15.248537063598633 ms\n",
      "\t=====[MLP]====== cpu->gpu IO 15.293598175048828 ms\n",
      "\t=====[MLP]====== gpu linear 0.2372264862060547 ms\n",
      "\t=====[MLP]====== gpu->cpu IO 5.251884460449219 ms\n",
      "\t=====[MLP]====== unmask xA 1.3010501861572266 ms\n",
      "\t=====[MLP]====== LoRA linear 3.3028125762939453 ms\n",
      "mlp 285.003662109375 ms\n",
      "NON_LINEAR_TIME\tIO_TIME\tMASK_TIME\tLINER_TIME_GPU\tLORA_TIME_TEE\n",
      "67.16346740722656\t118.33047866821289\t45.03273963928223\t1.2764930725097656\t22.70030975341797\t\n",
      "26.39%\t46.49%\t17.69%\t0.50%\t8.92%\t\n"
     ]
    }
   ],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_ctx, config, scale=False):\n",
    "        super(Block, self).__init__()\n",
    "        nx = config.n_embd\n",
    "        self.ln_1 = LayerNorm(nx, eps=config.layer_norm_epsilon)\n",
    "        self.attn = Attention(nx, n_ctx, config, scale)\n",
    "        self.ln_2 = LayerNorm(nx, eps=config.layer_norm_epsilon)\n",
    "        self.mlp = MLP(4 * nx, config)\n",
    "\n",
    "    def forward(self, x, layer_past=None, len_past=None):\n",
    "        start = time.time()\n",
    "        ln_res = self.ln_1(x)\n",
    "        end = time.time()\n",
    "        print(f\"ln_1 {(end - start) * 1000} ms\")\n",
    "        global NON_LINEAR_TIME\n",
    "        NON_LINEAR_TIME += (end - start) * 1000\n",
    "\n",
    "\n",
    "        start = time.time()\n",
    "        a, present = self.attn(ln_res, layer_past=layer_past, len_past=len_past)\n",
    "        print(f\"attention {(time.time() - start) * 1000} ms\")\n",
    "        \n",
    "        x = x + a\n",
    "\n",
    "        start = time.time()\n",
    "        ln_res = self.ln_2(x)\n",
    "        end = time.time()\n",
    "        print(f\"ln_2 {(end - start) * 1000} ms\")\n",
    "        NON_LINEAR_TIME += (end - start) * 1000\n",
    "\n",
    "        start = time.time()\n",
    "        m = self.mlp(ln_res)\n",
    "        print(f\"mlp {(time.time() - start) * 1000} ms\")\n",
    "\n",
    "        x = x + m\n",
    "        return x, present\n",
    "\n",
    "\n",
    "\n",
    "block = Block(config.n_ctx, config, scale=True)\n",
    "block.eval()\n",
    "\n",
    "hidden_states = torch.rand(size=(B, SEQ_LEN, config.n_embd))\n",
    "\n",
    "outputs = block(hidden_states)\n",
    "\n",
    "\n",
    "times = [NON_LINEAR_TIME, IO_TIME, MASK_TIME, LINER_TIME_GPU, LORA_TIME_TEE]\n",
    "total_time = sum(times)\n",
    "\n",
    "print(\"NON_LINEAR_TIME\\tIO_TIME\\tMASK_TIME\\tLINER_TIME_GPU\\tLORA_TIME_TEE\")\n",
    "abs_time = \"\"\n",
    "rel_time = \"\"\n",
    "for time_ in times:\n",
    "    abs_time += f\"{time_}\\t\"\n",
    "    rel_time += f\"{time_ / total_time * 100:.2f}%\\t\"\n",
    "print(abs_time)\n",
    "print(rel_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Attention using LoRA: PruneGPTConv1D(\n",
      "  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "MLP using LoRA: PruneGPTConv1D(\n",
      "  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "), PruneGPTConv1D(\n",
      "  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─LayerNorm: 1-1                         [-1, 512, 1024]           2,048\n",
      "├─Attention: 1-2                         [-1, 512, 1024]           --\n",
      "|    └─PruneLinear: 2-1                  [-1, 512, 1024]           --\n",
      "|    |    └─Dropout: 3-1                 [-1, 512, 1024]           --\n",
      "|    └─PruneLinear: 2-2                  [-1, 512, 1024]           --\n",
      "|    |    └─Dropout: 3-2                 [-1, 512, 1024]           --\n",
      "|    └─PruneLinear: 2-3                  [-1, 512, 1024]           --\n",
      "|    |    └─Dropout: 3-3                 [-1, 512, 1024]           --\n",
      "|    └─PruneGPTConv1D: 2-4               [-1, 512, 1024]           --\n",
      "|    |    └─Dropout: 3-4                 [-1, 512, 1024]           --\n",
      "├─LayerNorm: 1-3                         [-1, 512, 1024]           2,048\n",
      "├─MLP: 1-4                               [-1, 512, 1024]           --\n",
      "|    └─PruneGPTConv1D: 2-5               [-1, 512, 4096]           --\n",
      "|    |    └─Dropout: 3-5                 [-1, 512, 1024]           --\n",
      "|    └─PruneGPTConv1D: 2-6               [-1, 512, 1024]           --\n",
      "|    |    └─Dropout: 3-6                 [-1, 512, 4096]           --\n",
      "==========================================================================================\n",
      "Total params: 4,096\n",
      "Trainable params: 4,096\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 22.56\n",
      "==========================================================================================\n",
      "Input size (MB): 16.00\n",
      "Forward/backward pass size (MB): 8.00\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 24.02\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─LayerNorm: 1-1                         [-1, 512, 1024]           2,048\n",
       "├─Attention: 1-2                         [-1, 512, 1024]           --\n",
       "|    └─PruneLinear: 2-1                  [-1, 512, 1024]           --\n",
       "|    |    └─Dropout: 3-1                 [-1, 512, 1024]           --\n",
       "|    └─PruneLinear: 2-2                  [-1, 512, 1024]           --\n",
       "|    |    └─Dropout: 3-2                 [-1, 512, 1024]           --\n",
       "|    └─PruneLinear: 2-3                  [-1, 512, 1024]           --\n",
       "|    |    └─Dropout: 3-3                 [-1, 512, 1024]           --\n",
       "|    └─PruneGPTConv1D: 2-4               [-1, 512, 1024]           --\n",
       "|    |    └─Dropout: 3-4                 [-1, 512, 1024]           --\n",
       "├─LayerNorm: 1-3                         [-1, 512, 1024]           2,048\n",
       "├─MLP: 1-4                               [-1, 512, 1024]           --\n",
       "|    └─PruneGPTConv1D: 2-5               [-1, 512, 4096]           --\n",
       "|    |    └─Dropout: 3-5                 [-1, 512, 1024]           --\n",
       "|    └─PruneGPTConv1D: 2-6               [-1, 512, 1024]           --\n",
       "|    |    └─Dropout: 3-6                 [-1, 512, 4096]           --\n",
       "==========================================================================================\n",
       "Total params: 4,096\n",
       "Trainable params: 4,096\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 22.56\n",
       "==========================================================================================\n",
       "Input size (MB): 16.00\n",
       "Forward/backward pass size (MB): 8.00\n",
       "Params size (MB): 0.02\n",
       "Estimated Total Size (MB): 24.02\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.model import Block as RawBlock\n",
    "block = RawBlock(config.n_ctx, config, scale=True)\n",
    "block.eval()\n",
    "\n",
    "hidden_states = torch.rand(size=(B, SEQ_LEN, config.n_embd))\n",
    "summary(block, hidden_states, depth=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-qd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
