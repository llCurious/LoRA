{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path\n",
    "sys.path.append(\"/home/haoqi.whq/tee+gpu/LoRA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import GPT2Config, GPT2LMModel\n",
    "import torch\n",
    "from loralib import PruneLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Attention not use LoRA: Conv1D()\n",
      "MLP using LoRA: PruneGPTConv1D(\n",
      "  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "), PruneGPTConv1D(\n",
      "  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "loading model pretrained weight.\n",
      "transformer.h.0.attn.q_atten.lora_A\n",
      "transformer.h.0.attn.q_atten.lora_B\n",
      "transformer.h.0.attn.v_atten.lora_A\n",
      "transformer.h.0.attn.v_atten.lora_B\n",
      "transformer.h.0.mlp.c_fc.lora_A\n",
      "transformer.h.0.mlp.c_fc.lora_B\n",
      "transformer.h.0.mlp.c_proj.lora_A\n",
      "transformer.h.0.mlp.c_proj.lora_B\n",
      "transformer.h.1.attn.q_atten.lora_A\n",
      "transformer.h.1.attn.q_atten.lora_B\n",
      "transformer.h.1.attn.v_atten.lora_A\n",
      "transformer.h.1.attn.v_atten.lora_B\n",
      "transformer.h.1.mlp.c_fc.lora_A\n",
      "transformer.h.1.mlp.c_fc.lora_B\n",
      "transformer.h.1.mlp.c_proj.lora_A\n",
      "transformer.h.1.mlp.c_proj.lora_B\n",
      "transformer.h.2.attn.q_atten.lora_A\n",
      "transformer.h.2.attn.q_atten.lora_B\n",
      "transformer.h.2.attn.v_atten.lora_A\n",
      "transformer.h.2.attn.v_atten.lora_B\n",
      "transformer.h.2.mlp.c_fc.lora_A\n",
      "transformer.h.2.mlp.c_fc.lora_B\n",
      "transformer.h.2.mlp.c_proj.lora_A\n",
      "transformer.h.2.mlp.c_proj.lora_B\n",
      "transformer.h.3.attn.q_atten.lora_A\n",
      "transformer.h.3.attn.q_atten.lora_B\n",
      "transformer.h.3.attn.v_atten.lora_A\n",
      "transformer.h.3.attn.v_atten.lora_B\n",
      "transformer.h.3.mlp.c_fc.lora_A\n",
      "transformer.h.3.mlp.c_fc.lora_B\n",
      "transformer.h.3.mlp.c_proj.lora_A\n",
      "transformer.h.3.mlp.c_proj.lora_B\n",
      "transformer.h.4.attn.q_atten.lora_A\n",
      "transformer.h.4.attn.q_atten.lora_B\n",
      "transformer.h.4.attn.v_atten.lora_A\n",
      "transformer.h.4.attn.v_atten.lora_B\n",
      "transformer.h.4.mlp.c_fc.lora_A\n",
      "transformer.h.4.mlp.c_fc.lora_B\n",
      "transformer.h.4.mlp.c_proj.lora_A\n",
      "transformer.h.4.mlp.c_proj.lora_B\n",
      "transformer.h.5.attn.q_atten.lora_A\n",
      "transformer.h.5.attn.q_atten.lora_B\n",
      "transformer.h.5.attn.v_atten.lora_A\n",
      "transformer.h.5.attn.v_atten.lora_B\n",
      "transformer.h.5.mlp.c_fc.lora_A\n",
      "transformer.h.5.mlp.c_fc.lora_B\n",
      "transformer.h.5.mlp.c_proj.lora_A\n",
      "transformer.h.5.mlp.c_proj.lora_B\n",
      "transformer.h.6.attn.q_atten.lora_A\n",
      "transformer.h.6.attn.q_atten.lora_B\n",
      "transformer.h.6.attn.v_atten.lora_A\n",
      "transformer.h.6.attn.v_atten.lora_B\n",
      "transformer.h.6.mlp.c_fc.lora_A\n",
      "transformer.h.6.mlp.c_fc.lora_B\n",
      "transformer.h.6.mlp.c_proj.lora_A\n",
      "transformer.h.6.mlp.c_proj.lora_B\n",
      "transformer.h.7.attn.q_atten.lora_A\n",
      "transformer.h.7.attn.q_atten.lora_B\n",
      "transformer.h.7.attn.v_atten.lora_A\n",
      "transformer.h.7.attn.v_atten.lora_B\n",
      "transformer.h.7.mlp.c_fc.lora_A\n",
      "transformer.h.7.mlp.c_fc.lora_B\n",
      "transformer.h.7.mlp.c_proj.lora_A\n",
      "transformer.h.7.mlp.c_proj.lora_B\n",
      "transformer.h.8.attn.q_atten.lora_A\n",
      "transformer.h.8.attn.q_atten.lora_B\n",
      "transformer.h.8.attn.v_atten.lora_A\n",
      "transformer.h.8.attn.v_atten.lora_B\n",
      "transformer.h.8.mlp.c_fc.lora_A\n",
      "transformer.h.8.mlp.c_fc.lora_B\n",
      "transformer.h.8.mlp.c_proj.lora_A\n",
      "transformer.h.8.mlp.c_proj.lora_B\n",
      "transformer.h.9.attn.q_atten.lora_A\n",
      "transformer.h.9.attn.q_atten.lora_B\n",
      "transformer.h.9.attn.v_atten.lora_A\n",
      "transformer.h.9.attn.v_atten.lora_B\n",
      "transformer.h.9.mlp.c_fc.lora_A\n",
      "transformer.h.9.mlp.c_fc.lora_B\n",
      "transformer.h.9.mlp.c_proj.lora_A\n",
      "transformer.h.9.mlp.c_proj.lora_B\n",
      "transformer.h.10.attn.q_atten.lora_A\n",
      "transformer.h.10.attn.q_atten.lora_B\n",
      "transformer.h.10.attn.v_atten.lora_A\n",
      "transformer.h.10.attn.v_atten.lora_B\n",
      "transformer.h.10.mlp.c_fc.lora_A\n",
      "transformer.h.10.mlp.c_fc.lora_B\n",
      "transformer.h.10.mlp.c_proj.lora_A\n",
      "transformer.h.10.mlp.c_proj.lora_B\n",
      "transformer.h.11.attn.q_atten.lora_A\n",
      "transformer.h.11.attn.q_atten.lora_B\n",
      "transformer.h.11.attn.v_atten.lora_A\n",
      "transformer.h.11.attn.v_atten.lora_B\n",
      "transformer.h.11.mlp.c_fc.lora_A\n",
      "transformer.h.11.mlp.c_fc.lora_B\n",
      "transformer.h.11.mlp.c_proj.lora_A\n",
      "transformer.h.11.mlp.c_proj.lora_B\n",
      "transformer.h.12.attn.q_atten.lora_A\n",
      "transformer.h.12.attn.q_atten.lora_B\n",
      "transformer.h.12.attn.v_atten.lora_A\n",
      "transformer.h.12.attn.v_atten.lora_B\n",
      "transformer.h.12.mlp.c_fc.lora_A\n",
      "transformer.h.12.mlp.c_fc.lora_B\n",
      "transformer.h.12.mlp.c_proj.lora_A\n",
      "transformer.h.12.mlp.c_proj.lora_B\n",
      "transformer.h.13.attn.q_atten.lora_A\n",
      "transformer.h.13.attn.q_atten.lora_B\n",
      "transformer.h.13.attn.v_atten.lora_A\n",
      "transformer.h.13.attn.v_atten.lora_B\n",
      "transformer.h.13.mlp.c_fc.lora_A\n",
      "transformer.h.13.mlp.c_fc.lora_B\n",
      "transformer.h.13.mlp.c_proj.lora_A\n",
      "transformer.h.13.mlp.c_proj.lora_B\n",
      "transformer.h.14.attn.q_atten.lora_A\n",
      "transformer.h.14.attn.q_atten.lora_B\n",
      "transformer.h.14.attn.v_atten.lora_A\n",
      "transformer.h.14.attn.v_atten.lora_B\n",
      "transformer.h.14.mlp.c_fc.lora_A\n",
      "transformer.h.14.mlp.c_fc.lora_B\n",
      "transformer.h.14.mlp.c_proj.lora_A\n",
      "transformer.h.14.mlp.c_proj.lora_B\n",
      "transformer.h.15.attn.q_atten.lora_A\n",
      "transformer.h.15.attn.q_atten.lora_B\n",
      "transformer.h.15.attn.v_atten.lora_A\n",
      "transformer.h.15.attn.v_atten.lora_B\n",
      "transformer.h.15.mlp.c_fc.lora_A\n",
      "transformer.h.15.mlp.c_fc.lora_B\n",
      "transformer.h.15.mlp.c_proj.lora_A\n",
      "transformer.h.15.mlp.c_proj.lora_B\n",
      "transformer.h.16.attn.q_atten.lora_A\n",
      "transformer.h.16.attn.q_atten.lora_B\n",
      "transformer.h.16.attn.v_atten.lora_A\n",
      "transformer.h.16.attn.v_atten.lora_B\n",
      "transformer.h.16.mlp.c_fc.lora_A\n",
      "transformer.h.16.mlp.c_fc.lora_B\n",
      "transformer.h.16.mlp.c_proj.lora_A\n",
      "transformer.h.16.mlp.c_proj.lora_B\n",
      "transformer.h.17.attn.q_atten.lora_A\n",
      "transformer.h.17.attn.q_atten.lora_B\n",
      "transformer.h.17.attn.v_atten.lora_A\n",
      "transformer.h.17.attn.v_atten.lora_B\n",
      "transformer.h.17.mlp.c_fc.lora_A\n",
      "transformer.h.17.mlp.c_fc.lora_B\n",
      "transformer.h.17.mlp.c_proj.lora_A\n",
      "transformer.h.17.mlp.c_proj.lora_B\n",
      "transformer.h.18.attn.q_atten.lora_A\n",
      "transformer.h.18.attn.q_atten.lora_B\n",
      "transformer.h.18.attn.v_atten.lora_A\n",
      "transformer.h.18.attn.v_atten.lora_B\n",
      "transformer.h.18.mlp.c_fc.lora_A\n",
      "transformer.h.18.mlp.c_fc.lora_B\n",
      "transformer.h.18.mlp.c_proj.lora_A\n",
      "transformer.h.18.mlp.c_proj.lora_B\n",
      "transformer.h.19.attn.q_atten.lora_A\n",
      "transformer.h.19.attn.q_atten.lora_B\n",
      "transformer.h.19.attn.v_atten.lora_A\n",
      "transformer.h.19.attn.v_atten.lora_B\n",
      "transformer.h.19.mlp.c_fc.lora_A\n",
      "transformer.h.19.mlp.c_fc.lora_B\n",
      "transformer.h.19.mlp.c_proj.lora_A\n",
      "transformer.h.19.mlp.c_proj.lora_B\n",
      "transformer.h.20.attn.q_atten.lora_A\n",
      "transformer.h.20.attn.q_atten.lora_B\n",
      "transformer.h.20.attn.v_atten.lora_A\n",
      "transformer.h.20.attn.v_atten.lora_B\n",
      "transformer.h.20.mlp.c_fc.lora_A\n",
      "transformer.h.20.mlp.c_fc.lora_B\n",
      "transformer.h.20.mlp.c_proj.lora_A\n",
      "transformer.h.20.mlp.c_proj.lora_B\n",
      "transformer.h.21.attn.q_atten.lora_A\n",
      "transformer.h.21.attn.q_atten.lora_B\n",
      "transformer.h.21.attn.v_atten.lora_A\n",
      "transformer.h.21.attn.v_atten.lora_B\n",
      "transformer.h.21.mlp.c_fc.lora_A\n",
      "transformer.h.21.mlp.c_fc.lora_B\n",
      "transformer.h.21.mlp.c_proj.lora_A\n",
      "transformer.h.21.mlp.c_proj.lora_B\n",
      "transformer.h.22.attn.q_atten.lora_A\n",
      "transformer.h.22.attn.q_atten.lora_B\n",
      "transformer.h.22.attn.v_atten.lora_A\n",
      "transformer.h.22.attn.v_atten.lora_B\n",
      "transformer.h.22.mlp.c_fc.lora_A\n",
      "transformer.h.22.mlp.c_fc.lora_B\n",
      "transformer.h.22.mlp.c_proj.lora_A\n",
      "transformer.h.22.mlp.c_proj.lora_B\n",
      "transformer.h.23.attn.q_atten.lora_A\n",
      "transformer.h.23.attn.q_atten.lora_B\n",
      "transformer.h.23.attn.v_atten.lora_A\n",
      "transformer.h.23.attn.v_atten.lora_B\n",
      "transformer.h.23.mlp.c_fc.lora_A\n",
      "transformer.h.23.mlp.c_fc.lora_B\n",
      "transformer.h.23.mlp.c_proj.lora_A\n",
      "transformer.h.23.mlp.c_proj.lora_B\n"
     ]
    }
   ],
   "source": [
    "# medium GPT2\n",
    "config = GPT2Config(\n",
    "    n_embd=1024,\n",
    "    n_layer=24,\n",
    "    n_head=16,\n",
    "    lora_attn_dim=4,\n",
    "    lora_attn_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    enable_mlp=True,\n",
    "    enable_wo=False,\n",
    "    enable_wq=True,\n",
    "    enable_wk=False,\n",
    "    enable_wv=True,\n",
    ")\n",
    "\n",
    "lm_net = GPT2LMModel(config)\n",
    "print(\"loading model pretrained weight.\")\n",
    "lm_net.load_weight(torch.load(\"/home/haoqi.whq/tee+gpu/LoRA/examples/NLG/trained_models/GPT2_M/e2e/model.10000.pt\"))\n",
    "\n",
    "# for m in lm_net.modules():\n",
    "#     if isinstance(m, PruneLayer) and hasattr(m, \"scaling\"):\n",
    "#         print(m.scaling)\n",
    "    \n",
    "for n, p in lm_net.named_parameters():\n",
    "    if \"scaling\" in n:\n",
    "        print(n)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lora-tune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
